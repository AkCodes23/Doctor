# Doctor

> Doctor is a system that lets LLM agents discover, crawl, and index web sites for better and more up-to-date reasoning and code generation.

Doctor provides a complete stack for crawling, indexing, and searching web content to enhance LLM capabilities. It handles web crawling, text chunking, embedding generation, and semantic search, making this functionality available to large language models through a Model Context Protocol (MCP) server.

## Core Components

- [Crawl Worker](/src/crawl_worker) - Processes crawl jobs, chunks text, and creates embeddings
- [Web Service](/src/web_service) - FastAPI service exposing endpoints for fetching, searching, and viewing data, and exposing MCP server
- [Common](/src/common) - Shared code, models, and database utilities

## Infrastructure

- DuckDB - Database for storing document data and embeddings with vector search capabilities
- Redis - Message broker for asynchronous task processing
- Docker - Container orchestration for deploying the complete stack

## Models

- OpenAI Text Embeddings - Used for generating vector embeddings of text chunks
  - Implementation: text-embedding-ada-002 (1536 dimensions)
  - Integration: Accessed via litellm library

## Libraries

- crawl4ai - For web page crawling
- langchain_text_splitters - For chunking text content
- litellm - Wrapper for accessing embedding models
- fastapi - Web service framework
- fastapi-mcp - MCP server implementation
- duckdb-vss - Vector similarity search extension for DuckDB

## Technical Requirements

- Docker and Docker Compose - For running the complete stack
- Python 3.10+ - Primary programming language
- OpenAI API key - Required for embedding generation

## API Configuration

- OpenAI API key must be provided via environment variable: OPENAI_API_KEY
- Additional environment variables:
  - REDIS_URI - URI for Redis connection
  - DATA_DIR - Directory for storing DuckDB data files (default: "data")
